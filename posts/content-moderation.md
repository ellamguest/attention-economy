# Content moderation

## Taking down vs leaving up

## Amplification

Specifically algorithmic amplification that leads the content to reach a larger audience than it would have 'naturally'.

Clarify definitional distinction with virality.

Different from - but related to - content that is designed to be viral (often by being incendiary). Those processes obviously feed in to each other, but it's important to distinguish between technical mechanisms and content-based mechanisms.

Bots can be a form of amplification by users, rather than by the company

Algorithmic curation + catchy content (need a better term for this) ---> amplification/virality

## A free market of content moderation

Jurisdictions will set some hard limits. We already have some legal hard lines (CASM, copyright) and some semi legal semi social hard lines (adult materials, sexual solicitation).

New limits are developing and will continue to develop. Some will be progressive, so will be regressive.

But some subjects will never have bright lines of acceptability. For these I advocate a free market approach whereby users can 'vote with their feet'.

And that brings us back to competition and a more balanced product offering. I wees two approaches

1. a more diversified marketplace. Facebook, plus a more liberal minded version, plus a more right wing minded version. However I don't think this structure will work if their fully siloed, they'll be a need for some level of interoperability.

I think the dominant firms will fight that tooth and nails and it will even structural and technical changes that will be difficult to get right. Therefore I think the second option, at least in the shirt to medium term, is more practical. Perhaps it can be a gateway to the second?

2. Middleware layered on top.

Personalized algorithms based on user feedback. Already have 'i don't like this' but they're pretty weak mechanisms.

## Natural experiments

Content moderation may be the area in which there is the greatest diversity of regulatory approaches across jurisdictions, even within liberal democracies specifically.

On one hand there is the US where any regulation is limited by companies First Amendment right to free speech, which is further clarified by Section 230. Even transparency measures such as 'nutrition labels' could be unconstitutional because of the potential chilling effect on platform's speech. However the First Amendment is not as bright a line as it is sometimes made out to be, and the success or failure of specific pieces of regulation wil like depend more on the precedent set by existing case law and the interpretation of the Supreme court [cite douek & Lakier series].

Then we have the UK which has no similar free speech mandate. The only reference to freedom of expression is via EU human rights law (need to triple check this). The government is currently considering the Online Safety bill which would require platforms to moderate 'legal but harmful' content.

Then there's Australia where platforms face the same liability as publishers. See case where as politician successfully [sued Alphabet](https://www.cnbc.com/2022/06/06/google-ordered-to-pay-australian-politician-over-youtube-videos.html) for profiting from defamatory, racist YouTube videos.

## Connection to Privacy

I think there might be something in the connection between libel, defamation, tabloids, and invasion of privacy here. The UK has a notoriously invasive tabloid industry. But IIRC it also has stronger libel and defamation laws than the US. How does that hold? Is there some special allowance for media, 'national interest', etc?

[Content Moderation as Administration by Evelyn Douek :: SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4005326)